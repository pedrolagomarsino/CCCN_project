import numpy as np
from scipy.stats import norm
import matplotlib.pyplot as plt
import random

params = {'stim' : np.linspace(-50,50,21), # list of possible coherences/stimulus (sm in the model)
          'sigma_percept' : 10, # std of the gaussian used for perception
          'sigma_belief' : 10, #std of the gaussian used for belief
          'n_trials' : 1000, # number of trials
          'n_choices' : 2,
          'e' : .1, #learning rate
          'R' : [1,1.2,0], # [reward left,reward right, no reward]
          'threshold': .8, # decision threshold
          'dynamic_threshold': False,
          'Tmax': 100, # maximun amount of time-steps per trial
          'tau_decay' : 5, # decay time for the belief
          'th_var' : .001, # threshold for the convergence of Q
          'beta' : 10 # exploration parameter
}

## utils ##
def stimulus(params,sm):
    """sample an observation (sm_hat) given a stimulus (sm) and build a belief (bel) as in Lak paper"""
    sm_hat = np.random.normal(sm,params['sigma_percept'])
    bel = norm.pdf(params['stim'],sm_hat,params['sigma_belief'])
    bel = bel/sum(bel)
    return sm_hat,bel

def net_Q(a,Q,bel):
    """calculate net value of an action given a believe (bel), an action (a) and a Q-table (Q) """
    if a == 'R':
        netQ = sum(bel*Q[:,1])
    elif a == 'L':
        netQ = sum(bel*Q[:,0])
    else:
        print('Possible choices are R or L only')
    return netQ

## Initiate variables ##
Q    = np.zeros([len(params['stim']),params['n_choices']]) # state-action variable for each stimulus and action (expected cumulative reward)
Q_a  = np.zeros([params['n_trials'],params['n_choices']+1]) # save [QL,QR,choice]
QT   = np.zeros([params['Tmax'],params['n_choices'],params['n_trials']]) # Qtrial, the value of QR,QL in time and across trials
D_m  = np.zeros(params['n_trials']) # stimulus prediction error
D_fb = np.zeros(params['n_trials']) # feedback prediction error
Sm    = np.zeros(params['n_trials']) # stimulus for each trial
answers = np.zeros(params['n_trials']) # correct answers for each trial (L=-1 or R=1)
Time_steps = np.linspace(0,params['Tmax']-1,params['Tmax'])
if params['dynamic_threshold']:
    dyn_th = np.log(-(Time_steps-(params['Tmax'])))*params['threshold']/(np.log(params['Tmax'])) # dinamic threshold in time
else:
    dyn_th = np.ones(params['Tmax'])*params['threshold'] #flat threshold
    dyn_th[-1]=0
plt.plot(dyn_th)

trial = 0
#Initiate experiment#
for trial in range(params['n_trials']):
    sm = random.choice(params['stim']) #presented stimulus
    Qtrial = np.zeros([params['Tmax'],2])
    bel_cumulative = np.zeros([params['Tmax'],len(params['stim'])])
    # correct answer given stimulus
    if sm > 0:
        answer = 1
    elif sm < 0:
        answer = -1
    else:
        answer = random.choice([-1,1])
    # calculate State value befor stimulus presentation
    Vfc = .5*(net_Q('L',Q,np.ones(len(params['stim']))/len(params['stim'])) + net_Q('R',Q,np.ones(len(params['stim']))/len(params['stim'])))
    # after stimulus presentation enter the temporal loop
    p=0
    for t in range(params['Tmax']):
        sm_hat,bel = stimulus(params,sm) #make an observation at each time step

        # belief gets refine with every observation (belief is the product of all my previous beliefs)
        if t==0:
            bel_cumulative[t] = bel
        else:
            bel_cumulative[t] = bel_cumulative[t-1]*np.exp(-1/params['tau_decay'])*bel
        bel_cumulative[t] = bel_cumulative[t]/sum(bel_cumulative[t])

        # calculate net value of action
        QL = net_Q('L',Q,bel_cumulative[t])
        QR = net_Q('R',Q,bel_cumulative[t])
        Qtrial[t] = np.array([QL,QR])
        #Qtrial[t] = Qtrial[t-1]+np.array([QL,QR])/(t+1)  # integrate evidence, each step adds less info
        #Qtrial[t] = Qtrial[t-1]*np.exp(-1/params['tau_decay'])+np.array([QL,QR]) #

        # make a choice or move forward
        if max(Qtrial[t])<dyn_th[t]:
            # if QL,QR converged make a choice, otherwise continue
            d_Q = np.linalg.norm(Qtrial[t]-Qtrial[t-1])
            if d_Q<params['th_var']:
                p += 1
            if p>10:
                if Qtrial[t,0] == Qtrial[t,1]:
                    choice = random.choice([-1,1])
                else:
                    prob_L = 1/(1+np.exp(params['beta']*(Qtrial[t,1]-Qtrial[t,0])))
                    choice = np.array([1 if x>prob_L else 0 for x in np.random.rand(1)])*2-1
                    #choice = np.where(Qtrial[t] == np.amax(Qtrial[t]))[0][0]*2-1 # greedy choice (max)
                break
            continue
        else:
            if Qtrial[t,0] == Qtrial[t,1]:
                choice = random.choice([-1,1])
            else:
                prob_L = 1/(1+np.exp(params['beta']*(Qtrial[t,1]-Qtrial[t,0])))
                choice = np.array([1 if x>prob_L else 0 for x in np.random.rand(1)])*2-1
                #choice = np.where(Qtrial[t] == np.amax(Qtrial[t]))[0][0]*2-1 # greedy choice (max)
            break

    # calculate reward given choice
    if choice == answer:
        reward = params['R'][int((choice+1)/2)]
    else:
        reward = params['R'][2]

    # store variables for each trial
    Sm[trial]      = sm
    answers[trial] = answer
    QT[:,:,trial]  = Qtrial
    Q_a[trial,:2]  = Qtrial[t]
    Q_a[trial,2]   = int((choice+1)/2) # save binary choice
    delta_m        = Q_a[trial,choice]-Vfc
    D_m[trial]     = delta_m
    delta_fb       = reward - Q_a[trial,int((choice+1)/2)]
    D_fb[trial]    = delta_fb
    Q[:,int((choice+1)/2)] = Q[:,int((choice+1)/2)] + params['e']*delta_fb*bel_cumulative[t]# use the final belief to update Q

# The values of QR and QL converge to reward value
plt.figure()
c = 1
plt.plot(Q_a[Q_a[:,2]==c,c],'.')
c = 0
plt.plot(Q_a[Q_a[:,2]==c,c],'.')
plt.xlabel('trials')
plt.ylabel('Q_action')


plt.figure()
plt.plot(params['stim'],bel_cumulative.transpose())
plt.plot(params['stim'],bel_cumulative[-1])
#plt.plot(params['stim'],bel_cumulative[11])

# Competition between choices becomes faster with trials
tt = 300
plt.figure()
plt.plot(QT[:,0,tt], color='b')
plt.plot(QT[:,1,tt],color = 'r')
plt.plot(dyn_th)
plt.legend(['L','R','Th'])
plt.xlabel('time')
plt.ylabel('intra trial Q')
# Lak paper figure 1 plots
plt.figure()
plt.plot(params['stim'][-11:],[np.mean(D_m[np.logical_and(answers == Q_a[:,2], abs(Sm)==s)]) for s in params['stim'][-11:]])
plt.plot(params['stim'][-11:],[np.mean(D_m[np.logical_and(answers != Q_a[:,2], abs(Sm)==s)]) for s in params['stim'][-11:]])
plt.ylabel('delta_m')
plt.xlabel('coherence')

plt.figure()
plt.plot(params['stim'][-11:],[np.mean(D_fb[np.logical_and(answers == Q_a[:,2], abs(Sm)==s)]) for s in params['stim'][-11:]])
plt.plot(params['stim'][-11:],[np.mean(D_fb[np.logical_and(answers != Q_a[:,2], abs(Sm)==s)]) for s in params['stim'][-11:]])
plt.xlabel('coherence')
plt.ylabel('delfa_fb')
# time of decision decreases with trials, and is lower for bigger rewards
ttrials = np.linspace(0,params['n_trials']-1,params['n_trials']).astype(int)
t_des_r = [sum(QT[:,1,int(trial)]!=0) for trial in ttrials[Q_a[:,2]==1]]#range(params['n_trials']) ]
t_des_l = [sum(QT[:,0,int(trial)]!=0) for trial in ttrials[Q_a[:,2]==0]]
plt.figure()
plt.plot(t_des_r,'.')
plt.plot(t_des_l,'.')
plt.xlabel('trial')
plt.ylabel('desition time')
# Time to decision decreases with coherence and is lower for high rewards (result observed experimentally)
ttrials = np.linspace(0,params['n_trials']-1,params['n_trials']).astype(int)
tr_r = np.zeros(len(params['stim'][-11:]))
for i,s in enumerate(params['stim'][-11:]):
    tr_r[i] = np.mean([sum(QT[:,1,int(trial)]!=0) for trial in ttrials[np.array(Q_a[:,2]==1)*np.array(Sm==s)] ])
tr_l = np.zeros(len(params['stim'][-11:]))
for i,s in enumerate(params['stim'][:11]):
    tr_l[i] = np.mean([sum(QT[:,0,int(trial)]!=0) for trial in ttrials[np.array(Q_a[:,2]==0)*np.array(Sm==s)] ])
plt.figure()
plt.plot(params['stim'][-11:],tr_r)
plt.plot(abs(params['stim'][:11]),tr_l)
plt.xlabel('coherence')
plt.ylabel('mean decision time')
plt.legend(['R','L'])
#psicometric curve (amount of choices to the right) shifts depending on the reward size.
# to see this result one should run the code, then change the bias in reward and run it again
ttrials = np.linspace(500,params['n_trials']-1,params['n_trials']-500).astype(int)
Choice_R_high = [sum(Q_a[ttrials[Sm[500:]==s],2])/len(ttrials[Sm[500:]==s]) for s in params['stim']]
#Choice_R_low = [sum(Q_a[ttrials[Sm==s],2])/len(ttrials[Sm==s]) for s in params['stim']]
plt.figure()
plt.plot(params['stim'],Choice_R_high,'.-')
plt.xlabel('coherence')
plt.ylabel('Right choices')
#plt.plot(params['stim'],Choice_R_low,'.-')
